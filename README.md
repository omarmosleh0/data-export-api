# Data Export API

A production-grade FastAPI service for streaming large-scale database exports to Google Cloud Storage (GCS) with JWT authentication, designed for high-volume data operations.

## Overview

This API enables secure, paginated exports of PostgreSQL tables to both HTTP streams and GCS buckets. Built for big data scenarios where tables contain millions of rows and require efficient memory management.

## Key Features

###  **Core Functionality**

- **Streaming Exports**: Export tables as JSONL streams directly to HTTP response or GCS buckets
- **Zero Local Storage**: No temporary files - data streams from database directly to destination
- **Handles Big Data**: Designed for tables with millions of rows without memory overflow

###  **Scalability & Performance**

- **PK-Range Pagination**: Divide large tables into pages that can be exported in parallel by multiple workers
- **Memory-Efficient**: Uses server-side cursors - constant ~50MB memory usage regardless of table size
- **Configurable Limits**: Safety limits (100K rows/page default) prevent resource exhaustion
- **Connection Pooling**: Optimized PostgreSQL connection management with pre-ping health checks

###  **Security**

- **JWT Authentication**: OAuth2 bearer tokens validated against JWKS endpoint
- **SQL Injection Protection**: Three-layer defense:
    - Identifier validation (regex pattern matching)
    - Table allowlist (only registered tables exportable)
    - Parameterized queries (no string concatenation)
- **Rate Limiting**: 100 req/min per user for exports, 50 req/min for GCS uploads

###  **Cloud-Native Design**

- **Google Cloud Run**: Serverless deployment with auto-scaling (0-10 instances)
- **Cloud SQL Integration**: Unix socket connection via VPC - no public IP needed
- **GCS Direct Upload**: 16MB chunk streaming for optimal throughput (~100MB/s)
- **Secrets Manager**: Secure credential storage for DB passwords and JWT config

###  **Data Export Options**

1. **HTTP Streaming**: Download JSONL files directly via API response
2. **GCS Export**: Upload to cloud storage with organized folder structure:
    
    ```
    exports/â””â”€â”€ {table_name}/    â””â”€â”€ {YYYY-MM-DD}/        â”œâ”€â”€ page_1_of_10.jsonl        â”œâ”€â”€ page_2_of_10.jsonl        â””â”€â”€ ...
    ```
    

### ğŸ›¡ï¸ **Production-Ready Features**

- **Health Checks**: Liveness probe endpoint for Cloud Run monitoring
- **Structured Logging**: JSON logs with request tracing for Cloud Logging
- **Error Recovery**: Automatic JWKS cache refresh, graceful connection handling
- **Graceful Shutdown**: Proper connection pool disposal on container stop

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTPS/JWT      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   Cloud Run      â”‚
â”‚ (Authorized)â”‚                      â”‚  (FastAPI App)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                         â”‚                     â”‚
                    â–¼                         â–¼                     â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Cloud SQL    â”‚        â”‚     GCS      â”‚      â”‚     Issuer   â”‚
            â”‚ (PostgreSQL)  â”‚        â”‚   Buckets    â”‚      â”‚    (JWKS)    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
data-export-api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # FastAPI application & endpoints
â”‚   â”œâ”€â”€ database.py      # DB engine, streaming, GCS export logic
â”‚   â”œâ”€â”€ security.py      # JWT verification & user extraction
â”‚   â””â”€â”€ config.py        # Table registry configuration
â”œâ”€â”€ Dockerfile           # Container definition
â”œâ”€â”€ deploy.sh            # Cloud Run deployment script
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md           # This file
```

## Prerequisites

- **GCP Project**: Access to Google Cloud Platform with billing enabled
- **Cloud SQL**: PostgreSQL instance with Unix socket connection
- **GCS Bucket**: Pre-created bucket for exports (e.g., `data-exports-api`)
- **Issuer Account**: OIDC application configured with JWT issuance
- **Secrets Manager**: GCP Secret Manager configured with:
    - `db-password`: PostgreSQL password
    - `jwt-issuer`: issuer URL (e.g., `https://okta.com/oauth2/default`)
    - `jwt-audience`: JWT audience claim (e.g., `api://data-exports-api`)

## Installation & Setup

### 1. Clone Repository

```bash
git clone <repository-url>
cd data-export-api
```

### 2. Configure Table Registry

Edit `app/config.py` to define exportable tables:

```python
TABLE_REGISTRY = {
    "projects": {
        "name": "intern_projects",
        "columns": ["project_id", "project_name", "status", "assigned_to", "due_date"],
        "primary_key": "project_id"
    },
    "users": {
        "name": "user_accounts",
        "columns": ["user_id", "email", "created_at"],
        "primary_key": "user_id"
    }
}
```

**Security Note**: Only tables explicitly listed here can be exported. This acts as an allowlist to prevent unauthorized data access.

### 3. Local Development (Optional)

```bash
# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set environment variables
export DB_SOCKET_PATH="/cloudsql/PROJECT:REGION:INSTANCE"
export DB_NAME="postgres"
export DB_USER="postgres"
export DB_PASS="your-password"
export JWT_ISSUER="https://project.okta.com/oauth2/default"
export JWT_AUDIENCE="api://your-audience"

# Run locally
uvicorn app.main:app --reload --port 8080
```

### 4. Deploy to Cloud Run

```bash
# Build and push Docker image
gcloud builds submit --tag gcr.io/projectID/data-export-api

# Deploy with script
chmod +x deploy.sh
./deploy.sh
```

The deployment script handles:

- Cloud SQL instance attachment
- Environment variable configuration
- Secret mounting from Secret Manager
- Resource allocation (2GB RAM, 2 vCPU)
- Auto-scaling settings (0-10 instances)

## API Endpoints

### Health Check

```http
GET /
```

**Response:**

```json
{
  "status": "healthy",
  "service": "query-engine",
  "version": "2.0.0",
  "features": ["streaming_export", "gcs_export", "jwt_auth", "rate_limiting"]
}
```

### List Available Tables

```http
GET /tables
Authorization: Bearer <jwt-token>
```

**Response:**

```json
{
  "tables": [
    {
      "key": "projects",
      "name": "intern_projects",
      "columns": ["project_id", "project_name", "status"],
      "primary_key": "project_id"
    }
  ],
  "count": 1
}
```

### Stream Export (HTTP Response)

```http
GET /export/{table_key}?page=1&total_pages=10
Authorization: Bearer <jwt-token>
```

**Parameters:**

- `table_key`: Table identifier from registry (e.g., `projects`)
- `page`: Current page number (1-indexed)
- `total_pages`: Total pages to divide table into (1-500)

**Response:** JSONL stream with headers:

```
Content-Type: application/x-ndjson
Content-Disposition: attachment; filename=projects_page1_of_10.jsonl
X-Page-Number: 1
X-Total-Pages: 10
```

**Example:**

```bash
curl -H "Authorization: Bearer $TOKEN" \
  "https://data-export-api-xxx.run.app/export/projects?page=1&total_pages=10" \
  -o projects_page1.jsonl
```

### Export to GCS

```http
POST /export/{table_key}/gcs
Authorization: Bearer <jwt-token>
Content-Type: application/json

{
  "bucket_name": "data-export-api",
  "page": 1,
  "total_pages": 10,
  "folder_prefix": "exports"
}
```

**Response:**

```json
{
  "success": true,
  "message": "Export completed successfully",
  "export": {
    "gcs_uri": "gs://data-export-api/exports/projects/2025-12-26/page_1_of_10.jsonl",
    "public_url": "https://storage.googleapis.com/data-export-api/...",
    "rows_exported": 125000,
    "file_size_mb": 45.67,
    "page": 1,
    "total_pages": 10,
    "timestamp": "2025-12-26T14:30:00Z",
    "table_key": "projects",
    "content_type": "application/x-ndjson"
  }
}
```

## Authentication

### Obtaining JWT Token (Okta Example)

```bash
# Get token from Issuer
TOKEN=$(curl -X POST https://project.okta.com/oauth2/default/v1/token \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "grant_type=client_credentials" \
  -d "client_id=YOUR_CLIENT_ID" \
  -d "client_secret=YOUR_CLIENT_SECRET" \
  -d "scope=api.export" | jq -r '.access_token')

# Use token in requests
curl -H "Authorization: Bearer $TOKEN" \
  https://data-export-api-xxx.run.app/tables
```

### Token Requirements

- **Algorithm**: RS256 (RSA signature)
- **Required Claims**:
    - `iss`: Must match `JWT_ISSUER` environment variable
    - `aud`: Must match `JWT_AUDIENCE` environment variable
    - `exp`: Token expiration timestamp
    - `sub`: User identifier (used for rate limiting)
    - `kid`: Key ID in token header (for JWKS lookup)

## Pagination Strategy

The API uses **PK-range pagination** to ensure consistent, memory-efficient exports:

1. **Calculate Global Boundaries**: Query `MIN(pk)` and `MAX(pk)` from table
2. **Divide Range**: Split PK range into `total_pages` equal segments
3. **Query Page**: `WHERE pk BETWEEN page_start AND page_end ORDER BY pk`

### Example: 1M Rows, 10 Pages

```sql
-- Page 1: pk BETWEEN 1 AND 100000
-- Page 2: pk BETWEEN 100001 AND 200000
-- ...
-- Page 10: pk BETWEEN 900001 AND 1000000
```

### Benefits

- **Parallelizable**: Multiple workers can export different pages simultaneously
- **Memory-Safe**: Each page has a safety limit (default: 100K rows)
- **Resumable**: Failed pages can be re-exported without affecting others
- **Consistent**: No data duplication or gaps (unlike OFFSET-based pagination)

## Rate Limiting

- **Authenticated Users**: 100 requests/minute per user (based on JWT `sub` claim)
- **Unauthenticated/Invalid Tokens**: 100 requests/minute per IP address
- **GCS Exports**: 50 requests/minute per user (stricter due to resource intensity)

Rate limit headers in responses:

```
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 95
X-RateLimit-Reset: 1703608800
```

## Security Features

### SQL Injection Prevention

1. **Identifier Validation**: All table/column names validated with regex `^[a-zA-Z_][a-zA-Z0-9_]*$`
2. **Allowlist**: Only tables in `TABLE_REGISTRY` can be accessed
3. **Parameterized Queries**: All values passed via SQLAlchemy parameters
4. **Quoted Names**: Identifiers wrapped with `quoted_name()` for safe escaping

### Example Attack Prevention

```python
# âŒ This will be rejected (not in registry)
GET /export/users; DROP TABLE users--

# âŒ This will be rejected (invalid identifier)
GET /export/../../etc/passwd

# âœ… This is allowed (valid table in registry)
GET /export/projects
```

## Database Connection Management

### Connection Pool Configuration

```python
# Defaults (can override via env vars)
DB_POOL_SIZE = 10        # Connections to keep open
DB_MAX_OVERFLOW = 5      # Additional connections if pool exhausted
POOL_TIMEOUT = 10        # Seconds to wait for connection
POOL_RECYCLE = 1800      # Recycle connections after 30 min
POOL_PRE_PING = True     # Test connections before use
```

### Environment Variables

```bash
# Override defaults
export DB_POOL_SIZE=20
export DB_MAX_OVERFLOW=10
```

### Cloud SQL Connection

Uses Unix socket for secure, low-latency connection:

```
/cloudsql/PROJECT_ID:REGION:INSTANCE_NAME
```

No public IP required - connection routed through VPC connector.

## Performance Optimization

### Streaming Architecture

```python
# Server-side cursor (no memory buffering)
result = conn.execution_options(stream_results=True).execute(query)

# Yield rows one-at-a-time
for row in result:
    yield json.dumps(dict(row)) + "\n"
```

**Memory Usage**: ~50MB per request regardless of table size

### GCS Upload Strategy

```python
# 16MB chunks for optimal throughput
with blob.open(mode="w", chunk_size=16 * 1024 * 1024) as gcs_file:
    for line in data_stream:
        gcs_file.write(line)
```

**Bandwidth**: ~100MB/s sustained write speed to GCS

### Benchmarks (10M Row Table)

|Method|Time|Memory|Parallelizable|
|---|---|---|---|
|HTTP Stream (1 page)|~45s|50MB|âœ… Yes|
|GCS Export (1 page)|~50s|50MB|âœ… Yes|
|Full Export (10 workers)|~60s|500MB|âœ… Yes|

## Monitoring & Logging

### Cloud Logging Queries

```sql
-- Failed exports
resource.type="cloud_run_revision"
severity="ERROR"
jsonPayload.message=~"export"

-- Rate limit hits
resource.type="cloud_run_revision"
jsonPayload.message=~"Rate limit exceeded"

-- Slow queries (>30s)
resource.type="cloud_run_revision"
jsonPayload.duration>30
```

### Metrics to Monitor

- **Request Latency**: `run.googleapis.com/request_latencies` (target: p95 < 60s)
- **Instance Count**: `run.googleapis.com/container/instance_count` (watch auto-scaling)
- **Database Connections**: Custom metric from connection pool stats
- **GCS Write Throughput**: `storage.googleapis.com/api/request_count`

## Troubleshooting

### Issue: "DB engine not initialized"

**Cause**: Engine singleton not created during startup

**Fix**: Ensure `@app.on_event("startup")` calls `init_db_engine()` before first request

### Issue: "Public key not found for kid=abc123"

**Cause**: Issuer rotated signing keys, JWKS cache stale

**Fix**: Cache auto-refreshes on key miss. If persistent, check Issuer JWKS endpoint availability

### Issue: "Operation would exceed quota"

**Cause**: Too many concurrent GCS writes or database connections

**Fix**:

- Reduce `DB_POOL_SIZE` and `DB_MAX_OVERFLOW`
- Decrease `max-instances` in Cloud Run deployment
- Increase request timeout if exports are slow

### Issue: "Memory limit exceeded"

**Cause**: `safety_limit` too high or unexpected data volume

**Fix**:

- Reduce safety limit in `stream_table_jsonl()` (default: 100K rows)
- Increase Cloud Run memory allocation (currently 2GB)
- Use more pages to reduce per-page row count

## Development Workflow

### Adding a New Table

1. **Update Registry** (`config.py`):

```python
TABLE_REGISTRY["new_table"] = {
    "name": "actual_table_name",
    "columns": ["id", "name", "created_at"],
    "primary_key": "id"
}
```

2. **Test Locally**:

```bash
curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:8080/export/new_table?page=1&total_pages=1 \
  -o test.jsonl
```

3. **Deploy**:

```bash
./deploy.sh
```
